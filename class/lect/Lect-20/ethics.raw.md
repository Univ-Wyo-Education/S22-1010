
<!--
https://www.nature.com/articles/s41599-020-0501-9

https://www.weforum.org/agenda/2016/10/top-10-ethical-issues-in-artificial-intelligence/
-->




# Ethics

Final - composed of some simple programming questions
and... a short essay question(s) on ML/AI - focused on usage of ML/AI and
ethics.  It will be a take home.  It will be available by Dec 6 - the week 
before finals -and- due on Dec 15th at midnight.   Unlike the midterm
the due-date is pretty much a hard-deadline.

Last Lab - will be due Dec 6.  Classification of images.


## Privacy and Surveillance

Do you have a rite to secrecy?

Humans are "leaking" data to our devices all the time.   Location data, for example, is 
incredibly powerful.

1. Location data that shows that a person has a particular behavior.
In this case I am thinking of a Catholic Bishop that was outed for is
sexual orientation using location data.
2. Location data used by "stalkers" to track victims.

Apps that you install can get this data.   Sometimes intentionally.  Sometimes by function.
A iPhone "flashlight" that tracks your location.  Or an employer tracking device that
determines where you are when you are at work - but also sells the data of where you
are when all the time.

Financial data.

1. Payment transfer data that is used to show that a spouse is cheating.
2. Payment data that shows that a person is at a location at a certain time.

App Data - what apps you use.

Battery data - what is the level of charge of your phone.

Cameras watching you.  Facial recognition.  Gate and pace recognition.


"The data trail we leave behind is how our “free” services are paid
for—but we are not told about that data collection and the value
of this new raw material, and we are manipulated into leaving ever
more such data. For the “big 5” companies (Amazon, Google/Alphabet,
Microsoft, Apple, Facebook), the main data-collection part of their
business appears to be based on deception, exploiting human weaknesses,
furthering procrastination, generating addiction, and manipulation
(Harris 2016 [OIR]). The primary focus of social media, gaming, and
most of the Internet in this “surveillance economy” is to gain,
maintain, and direct attention—and thus data supply. “Surveillance
is the business model of the Internet” (Schneier 2015). This
surveillance and attention economy is sometimes called “surveillance
capitalism” (Zuboff 2019)."

[https://plato.stanford.edu/entries/ethics-ai/](https://plato.stanford.edu/entries/ethics-ai/)
(Read for Final!)

All of us has a "native" that we want to tell other people that is not really our life.
How much of your real "life" do you want to have revealed.

1. Credit scores are used to determine what you can rent (apartments, cars, etc) what jobs you get.
2. Social media data collection can determine vast information about you...  A picture of you at a
party will often contain GPS location data in the image, and identify your behavior.  An car insurance
company can use social media data to determine your risk of being a drunk driver based on images
of you.  For example - a ML that looks for Red Solo Cups at a party is a predictor of drunken
behavior.

## Derived data 

Derived data - and thereby derived ability to manipulate - is the real thereat.

![vizio.png](vizio.png)

[https://www.engadget.com/vizio-q1-earnings-inscape-013937337.html](https://www.engadget.com/vizio-q1-earnings-inscape-013937337.html)
(skim for final)

These systems combined with machine learning know more about us than we know about ourselves.


##  Manipulation of Behavior

Design of systems has become heavily influenced by the ability to produce "addition" to the interface.
The designer of the Facebook Interface has a book called, "Hooked: How to Build Habit-Forming Products".

Manipulation of the user - so as to maximize profit - or maximize attention has become a core business
model for a large portion of the Internet. 

For example....  It has been really clearly shown that to be productive in working on some task you
need chunks of un-interrupted time.  Having a "bell" that rings, or a pop-up message on your phone
generally grantees that you will be interrupted.  So let's create a bunch of different 
communication tools, Email, Slack, Discord, and expect that the people that we work with 
respond to all of them all the time.  

Social media is the primary source of political propaganda.  This influence can be used to alter users
voting habits.  Validation of what users see based on what is "true" or accurate is non-existent.
The only validation that is performed is a ML determining what is likely to get the most attention
and generate the most advertising revenue.

## Opacity of AI/ML systems.

Somebody asked if we can determine how the ML reached it's conclusion.   That is serious concern.
EU data privacy laws have been changed to make this a requirement but ... and this is a big but ...
how do you actually determine this.  Also ML systems that learn on an ongoing basis - what it
will do on one day is not the same as on another day.

If an AI/ML system is determining important things about our future - making assessments of "risk"
or assessments of "value" - then do we have a right to know how/what it is assessing.

Henry Kissinger, former Secretary of State,  has just written a book - 98 years old and still 
working on stuff - that points out the fundamental problem for democratic decision-making - if
we rely on a system that can and will influence our attitudes - do we have a right to have an
explanation of what that system is doing.  Do we need get explanations of its decisions.

### Decisions Support Systems

1. The Halo effect - if you use a support system you often can not change or influence the outcome.
2. Laziness - why not just accept what the support system provides.

### Automation Distrust Bias

1. How can a "system" produce better results than a person.
2. Training is only as good as the data.  Was the data any good?


## Bias

Systems are trained on data from the past.  So if in the past there was a "bias" in the
system then the new automated system will have that.  Amazon used a ML system to filter for
hiring - only to discover that the system discriminated again women.  They pulled the automated
system.  But think about this - if the system was trained on the prior practices of hiring then
Amazon already had discrimination again women.  The system was revealing about the data that
it was trained on.


